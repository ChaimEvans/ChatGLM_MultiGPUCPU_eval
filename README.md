# ChatGLM_MultiGPUCPU_eval
ç®€æ˜“å®ç°ChatGLMå•æœºè°ƒç”¨å¤šä¸ªè®¡ç®—è®¾å¤‡ï¼ˆGPUã€CPUï¼‰è¿›è¡Œæ¨ç†
## æ¨ç†
### 1.ä»ä»“åº“ä¸‹è½½ MultiDevices.py
### 2.åŠ è½½æ¨¡å‹
```python
>>> from transformers import AutoTokenizer, AutoModel
>>> tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True)
>>> model = AutoModel.from_pretrained("THUDM/chatglm-6b", trust_remote_code=True).half()
```
### 3.è°ƒç”¨å‡½æ•°
```python
>>> import MultiDevices
>>> model = MultiDevices.ConfigMultiDevices(model,
                                            embeddings='cpu',
                                            layers={
                                                    'cuda:0': '1-14',
                                                    'cuda:1': '15-28'
                                                   },
                                            final_layernorm='cuda:1')
word_embeddings -> cuda:1
layer 0 -> cuda:0
layer 1 -> cuda:0
layer 2 -> cuda:0
layer 3 -> cuda:0
layer 4 -> cuda:0
layer 5 -> cuda:0
layer 6 -> cuda:0
layer 7 -> cuda:0
layer 8 -> cuda:0
layer 9 -> cuda:0
layer 10 -> cuda:0
layer 11 -> cuda:0
layer 12 -> cuda:0
layer 13 -> cuda:0
layer 14 -> cuda:1
layer 15 -> cuda:1
layer 16 -> cuda:1
layer 17 -> cuda:1
layer 18 -> cuda:1
layer 19 -> cuda:1
layer 20 -> cuda:1
layer 21 -> cuda:1
layer 22 -> cuda:1
layer 23 -> cuda:1
layer 24 -> cuda:1
layer 25 -> cuda:1
layer 26 -> cuda:1
layer 27 -> cuda:1
final_layernorm -> cuda:1
lm_head -> cuda:1
hooked.
```
```python
>>> model = model.eval()
>>> response, history = model.chat(tokenizer, "ä½ å¥½", history=[])
>>> print(response)
ä½ å¥½ğŸ‘‹!æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ ChatGLM-6B,å¾ˆé«˜å…´è§åˆ°ä½ ,æ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
```
## å‡½æ•°è¯´æ˜
![ç»“æ„](structure.jpg)
```python
MultiDevices.ConfigMultiDevices(model,
                                embeddings='device',
                                layers={
                                        'device': 'layer_id-layer_id',
                                        'device': 'layer_id-layer_id',
                                        '......': '.................'
                                       },
                                final_layernorm='device')
```
## å·²æµ‹è¯•
### FP16
#### 8G GPU + 8G GPU
> NVIDIA Tesla P4 + NVIDIA P104-100

```python
embeddings='cuda:0',
layers={
        'cuda:0': '1-14',
        'cuda:1': '15-28'
       },
final_layernorm='cuda:1')
```
#### 8G GPU + 8G GPU + CPU
> NVIDIA Tesla P4 + NVIDIA P104-100
```python
embeddings='cpu',
layers={
        'cuda:0': '1-14',
        'cuda:1': '15-28'
       },
final_layernorm='cuda:1'
```
### INT8
#### 8G GPU + CPU
> NVIDIA Tesla P4
```python
æ­£åœ¨æµ‹è¯•
```
### INT4
#### 4G GPU + CPU
> NVIDIA Tesla P4 4G ï¼ˆå…³é—­above 4gï¼‰
```python
embeddings='cpu',
layers={
        'cuda:0': '1-24',
        'cpu': '25-28'
       },
final_layernorm='cpu'
```
è¯·ä½¿ç”¨å·²é‡åŒ–çš„æ¨¡å‹ï¼Œå¹¶ç¡®è®¤CPU Kernelç¼–è¯‘æˆåŠŸ
